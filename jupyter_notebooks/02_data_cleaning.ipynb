{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **(Retail sales Data cleaning and preparation)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Perform basic cleaning and preparation of the verified Superstore Sales dataset. \n",
        "* Handle missing values , fix incorrect data types and remove duplicates. \n",
        "* Save a cleaned version of the dataset to the data/processed directory. \n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Verified dataset: data/raw/\n",
        "  sales_raw_varified.csv\n",
        "\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Cleaned dataset: data/processed/\n",
        "  sales_cleaned.csv. \n",
        "\n",
        "## Additional Comments\n",
        "This notebook focuses only on cleaning and formatting the dataset for analysis. \n",
        "\n",
        "* No visualisations or feature engineering are performed here \n",
        "* The cleaned dataset will be used in the next notebook for exloration and analysis. \n",
        "* If you have any additional comments that don't fit in the previous bullets, please state them here. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import required libraries \n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "# set visualisation style \n",
        "sns.set(style=\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## load the verified dataset \n",
        "I start by loading the verified dataset from the 'data/raw' directory and preview the first few rows to confirm successful import. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Row ID</th>\n",
              "      <th>Order ID</th>\n",
              "      <th>Order Date</th>\n",
              "      <th>Ship Date</th>\n",
              "      <th>Ship Mode</th>\n",
              "      <th>Customer ID</th>\n",
              "      <th>Customer Name</th>\n",
              "      <th>Segment</th>\n",
              "      <th>Country</th>\n",
              "      <th>City</th>\n",
              "      <th>State</th>\n",
              "      <th>Postal Code</th>\n",
              "      <th>Region</th>\n",
              "      <th>Product ID</th>\n",
              "      <th>Category</th>\n",
              "      <th>Sub-Category</th>\n",
              "      <th>Product Name</th>\n",
              "      <th>Sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>CA-2017-152156</td>\n",
              "      <td>08/11/2017</td>\n",
              "      <td>11/11/2017</td>\n",
              "      <td>Second Class</td>\n",
              "      <td>CG-12520</td>\n",
              "      <td>Claire Gute</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>Henderson</td>\n",
              "      <td>Kentucky</td>\n",
              "      <td>42420.0</td>\n",
              "      <td>South</td>\n",
              "      <td>FUR-BO-10001798</td>\n",
              "      <td>Furniture</td>\n",
              "      <td>Bookcases</td>\n",
              "      <td>Bush Somerset Collection Bookcase</td>\n",
              "      <td>261.9600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>CA-2017-152156</td>\n",
              "      <td>08/11/2017</td>\n",
              "      <td>11/11/2017</td>\n",
              "      <td>Second Class</td>\n",
              "      <td>CG-12520</td>\n",
              "      <td>Claire Gute</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>Henderson</td>\n",
              "      <td>Kentucky</td>\n",
              "      <td>42420.0</td>\n",
              "      <td>South</td>\n",
              "      <td>FUR-CH-10000454</td>\n",
              "      <td>Furniture</td>\n",
              "      <td>Chairs</td>\n",
              "      <td>Hon Deluxe Fabric Upholstered Stacking Chairs,...</td>\n",
              "      <td>731.9400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>CA-2017-138688</td>\n",
              "      <td>12/06/2017</td>\n",
              "      <td>16/06/2017</td>\n",
              "      <td>Second Class</td>\n",
              "      <td>DV-13045</td>\n",
              "      <td>Darrin Van Huff</td>\n",
              "      <td>Corporate</td>\n",
              "      <td>United States</td>\n",
              "      <td>Los Angeles</td>\n",
              "      <td>California</td>\n",
              "      <td>90036.0</td>\n",
              "      <td>West</td>\n",
              "      <td>OFF-LA-10000240</td>\n",
              "      <td>Office Supplies</td>\n",
              "      <td>Labels</td>\n",
              "      <td>Self-Adhesive Address Labels for Typewriters b...</td>\n",
              "      <td>14.6200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>US-2016-108966</td>\n",
              "      <td>11/10/2016</td>\n",
              "      <td>18/10/2016</td>\n",
              "      <td>Standard Class</td>\n",
              "      <td>SO-20335</td>\n",
              "      <td>Sean O'Donnell</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>Fort Lauderdale</td>\n",
              "      <td>Florida</td>\n",
              "      <td>33311.0</td>\n",
              "      <td>South</td>\n",
              "      <td>FUR-TA-10000577</td>\n",
              "      <td>Furniture</td>\n",
              "      <td>Tables</td>\n",
              "      <td>Bretford CR4500 Series Slim Rectangular Table</td>\n",
              "      <td>957.5775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>US-2016-108966</td>\n",
              "      <td>11/10/2016</td>\n",
              "      <td>18/10/2016</td>\n",
              "      <td>Standard Class</td>\n",
              "      <td>SO-20335</td>\n",
              "      <td>Sean O'Donnell</td>\n",
              "      <td>Consumer</td>\n",
              "      <td>United States</td>\n",
              "      <td>Fort Lauderdale</td>\n",
              "      <td>Florida</td>\n",
              "      <td>33311.0</td>\n",
              "      <td>South</td>\n",
              "      <td>OFF-ST-10000760</td>\n",
              "      <td>Office Supplies</td>\n",
              "      <td>Storage</td>\n",
              "      <td>Eldon Fold 'N Roll Cart System</td>\n",
              "      <td>22.3680</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
              "0       1  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
              "1       2  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
              "2       3  CA-2017-138688  12/06/2017  16/06/2017    Second Class    DV-13045   \n",
              "3       4  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
              "4       5  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
              "\n",
              "     Customer Name    Segment        Country             City       State  \\\n",
              "0      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
              "1      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
              "2  Darrin Van Huff  Corporate  United States      Los Angeles  California   \n",
              "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
              "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
              "\n",
              "   Postal Code Region       Product ID         Category Sub-Category  \\\n",
              "0      42420.0  South  FUR-BO-10001798        Furniture    Bookcases   \n",
              "1      42420.0  South  FUR-CH-10000454        Furniture       Chairs   \n",
              "2      90036.0   West  OFF-LA-10000240  Office Supplies       Labels   \n",
              "3      33311.0  South  FUR-TA-10000577        Furniture       Tables   \n",
              "4      33311.0  South  OFF-ST-10000760  Office Supplies      Storage   \n",
              "\n",
              "                                        Product Name     Sales  \n",
              "0                  Bush Somerset Collection Bookcase  261.9600  \n",
              "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400  \n",
              "2  Self-Adhesive Address Labels for Typewriters b...   14.6200  \n",
              "3      Bretford CR4500 Series Slim Rectangular Table  957.5775  \n",
              "4                     Eldon Fold 'N Roll Cart System   22.3680  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load the verified dataset \n",
        "data_path ='../data/raw/sales_raw_varified.csv'\n",
        "df= pd.read_csv(data_path)\n",
        "# preview first 5 rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect dataset structure and basic Information\n",
        "I confirm the dataset shape, column names and data types. \n",
        "This step helps identify  missing values, duplicates and understand the dataset's structure before cleaning. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of the dataset: (9800, 18)\n",
            "colunmsin the dataset: ['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales']\n",
            "data types of the columns:\n",
            " Row ID             int64\n",
            "Order ID          object\n",
            "Order Date        object\n",
            "Ship Date         object\n",
            "Ship Mode         object\n",
            "Customer ID       object\n",
            "Customer Name     object\n",
            "Segment           object\n",
            "Country           object\n",
            "City              object\n",
            "State             object\n",
            "Postal Code      float64\n",
            "Region            object\n",
            "Product ID        object\n",
            "Category          object\n",
            "Sub-Category      object\n",
            "Product Name      object\n",
            "Sales            float64\n",
            "dtype: object\n",
            "missing values in each column::\n",
            " Row ID            0\n",
            "Order ID          0\n",
            "Order Date        0\n",
            "Ship Date         0\n",
            "Ship Mode         0\n",
            "Customer ID       0\n",
            "Customer Name     0\n",
            "Segment           0\n",
            "Country           0\n",
            "City              0\n",
            "State             0\n",
            "Postal Code      11\n",
            "Region            0\n",
            "Product ID        0\n",
            "Category          0\n",
            "Sub-Category      0\n",
            "Product Name      0\n",
            "Sales             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# check structure of the dataset\n",
        "print (\"shape of the dataset:\",df.shape)\n",
        "print (\"colunmsin the dataset:\",list(df.columns))\n",
        "print(\"data types of the columns:\\n\",df.dtypes)\n",
        "# check for missing values \n",
        "print(\"missing values in each column::\\n\", df.isnull().sum())\n",
        "      \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Here i Identify missing values in each column and check for duplicate records in the dataset to ensure data quality. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "missing values in each column after filling:\n",
            " Row ID           0\n",
            "Order ID         0\n",
            "Order Date       0\n",
            "Ship Date        0\n",
            "Ship Mode        0\n",
            "Customer ID      0\n",
            "Customer Name    0\n",
            "Segment          0\n",
            "Country          0\n",
            "City             0\n",
            "State            0\n",
            "Postal Code      0\n",
            "Region           0\n",
            "Product ID       0\n",
            "Category         0\n",
            "Sub-Category     0\n",
            "Product Name     0\n",
            "Sales            0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Handle missing values \n",
        "# fill missing Postal code values with the most frequent one (mode)\n",
        "df['Postal Code'].fillna(df['Postal Code'].mode()[0], inplace=True)\n",
        "\n",
        "#Confirm missing values after filling:\\n\", df.isnull().sum()\n",
        "print(\"missing values in each column after filling:\\n\", df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Covert data types \n",
        " I convert the 'order Date and 'Ship Date' columns to datetime format for easier analysis of sales and shipping trends later. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "updated data types:\n",
            " Order Date    datetime64[ns]\n",
            "Ship Date     datetime64[ns]\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# convert date column to datetime format \n",
        "df['Order Date'] = pd.to_datetime(df['Order Date'], errors='coerce')\n",
        "df['Ship Date'] = pd.to_datetime(df['Ship Date'], errors='coerce')\n",
        "   \n",
        "   # Confirm the date types changed \n",
        "print (\"updated data types:\\n\",df.dtypes[['Order Date', 'Ship Date']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rename columns for consistency \n",
        "I rename columns to use underscores instead of spaces and ensure consistent naming conventions which improves readability and avoids syntax issues in python. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated column names:\n",
            " Index(['Row_ID', 'Order_ID', 'Order_Date', 'Ship_Date', 'Ship_Mode',\n",
            "       'Customer_ID', 'Customer_name', 'Segment', 'Country', 'City', 'State',\n",
            "       'Postal_Code', 'Region', 'Product_ID', 'Category', 'Sub_Category',\n",
            "       'Product_Name', 'Sales'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Rename columns to use consistent naming convention \n",
        "df.rename(columns={\n",
        "    'Row ID': 'Row_ID',\n",
        "    'Order ID': 'Order_ID',\n",
        "    'Order Date': 'Order_Date', \n",
        "    'Ship Date': 'Ship_Date', \n",
        "    'Ship Mode': 'Ship_Mode', \n",
        "    'Customer ID': 'Customer_ID',\n",
        "    'Customer Name': 'Customer_name', \n",
        "    'Postal Code': 'Postal_Code',\n",
        "    'Product ID': 'Product_ID', \n",
        "    'Product Name': 'Product_Name', \n",
        "    'Sub-Category': 'Sub_Category'\n",
        "    },inplace=True)\n",
        "\n",
        "# Confirm column names changed \n",
        "print (\"Updated column names:\\n\", df.columns)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned dataset prepared and ready for analysis.\n",
            "Shape: (9800, 18)\n",
            "Columns: ['Row_ID', 'Order_ID', 'Order_Date', 'Ship_Date', 'Ship_Mode', 'Customer_ID', 'Customer_name', 'Segment', 'Country', 'City', 'State', 'Postal_Code', 'Region', 'Product_ID', 'Category', 'Sub_Category', 'Product_Name', 'Sales']\n"
          ]
        }
      ],
      "source": [
        "# keep only these columns \n",
        "columns_to_keep = [\n",
        "      'Row_ID',  'Order_ID',  'Order_Date',    'Ship_Date',   'Ship_Mode',    \n",
        "      'Customer_ID',   'Customer_name',  'Segment', 'Country',    'City',  'State',        \n",
        "      'Postal_Code', 'Region',  'Product_ID',  'Category',  'Sub_Category', \n",
        "      'Product_Name', 'Sales',      \n",
        "]\n",
        "\n",
        "df = df[columns_to_keep]\n",
        "\n",
        "# Drop duplicates\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "print(\"Cleaned dataset prepared and ready for analysis.\")\n",
        "print (\"Shape:\", df.shape)\n",
        "print (\"Columns:\",df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Row_ID', 'Order_ID', 'Order_Date', 'Ship_Date', 'Ship_Mode', 'Customer_ID', 'Customer_name', 'Segment', 'Country', 'City', 'State', 'Postal_Code', 'Region', 'Product_ID', 'Category', 'Sub_Category', 'Product_Name', 'Sales']\n"
          ]
        }
      ],
      "source": [
        "print(df.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the cleaned dataset\n",
        "I save the cleaned version of the dataset into the '/data/processed' folder as 'cleaned_sales_sata.csv' for use in the analysis and visualisation notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cleaned dataset saved to: ../data/processed/cleaned_sales_data.csv\n"
          ]
        }
      ],
      "source": [
        "# Save the cleaned dataset to a new CSV file \n",
        "output_path ='../data/processed/cleaned_sales_data.csv'\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print (\"cleaned dataset saved to:\",output_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify the saved cleaned dataset\n",
        "Here, I confirm that the cleaned dataset has be successfully saved to the correct folder and contains the expected number of rows and columns. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cleaned dataset saved successfully at: ../data/processed/cleaned_sales_data.csv\n",
            "raws and columns in the cleaned dataset: (9800, 18)\n"
          ]
        }
      ],
      "source": [
        "# Verify that the cleaned dataset is saved correctly \n",
        "import os \n",
        "import pandas as pd \n",
        "\n",
        "processed_path ='../data/processed/cleaned_sales_data.csv'\n",
        "\n",
        "if os.path.exists(processed_path):\n",
        "    print(\"cleaned dataset saved successfully at:\", processed_path)\n",
        "    df_check= pd.read_csv(processed_path)\n",
        "    print(\"raws and columns in the cleaned dataset:\", df_check.shape)\n",
        "else:\n",
        "    print(\"cleaned dataset not found.Please check the file path.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current directory before change: c:\\Projects\\retail-sales-analytics-capstone\\jupyter_notebooks\n",
            "current directory after change: c:\\Projects\\retail-sales-analytics-capstone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# get the current working directory \n",
        "current_dir = os.getcwd()\n",
        "print (\"current directory before change:\", current_dir)\n",
        "\n",
        "# change to the parent directory (project root)\n",
        "os.chdir(os.path.dirname(current_dir))\n",
        "print (\"current directory after change:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Projects\\\\retail-sales-analytics-capstone'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# confirm final working directory \n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Conclusions and next steps \n",
        "\n",
        "### Summary \n",
        "In this book , I have successsfully cleaned and prepared the varified superstore Sales datasetfor analysis. \n",
        "\n",
        "- Loaded and inspected the varified dataset from 'data/raw/sales_raw_varified.csv'.\n",
        "- Checked and handled missing values in key columns (e.g.'Postal Code').\n",
        "- Converted 'Order Sate' and 'Ship DAte'  to datetime format. \n",
        "- Renamed columns for consistency and readability. \n",
        "- Varified and saved the cleaned dataset as 'data/processed/cleaned_sales_data.csv'.\n",
        "- Confirmed the file was saved successfully and updated the working directory. \n",
        "\n",
        "### Next Steps \n",
        "The cleaned dataset is now ready for **Exploratory Data Analysis (EDA)**. \n",
        "- Load the cleaned dataset. \n",
        "- Explore sales trends and customer behavior. \n",
        "- Visualise insights across regions , categories and time. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
